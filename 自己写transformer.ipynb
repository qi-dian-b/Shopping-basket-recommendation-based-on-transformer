{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model stracture ####\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=512, num_layers=1, dropout=0):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=8, dropout=dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.decoder = nn.Linear(feature_size, 1)\n",
    "        \n",
    "        self.init_weights() # nn.Linear初始化\n",
    "        \n",
    "        self.src_key_padding_mask = None #解码器的mask是空 可以直接删除\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src,src_key_padding_mask):\n",
    "        src_key_padding_mask = src_key_padding_mask.bool()\n",
    "        \n",
    "        src = self.pos_encoder(src) # 位置编码 \n",
    "        \n",
    "        output = self.transformer_encoder(src,self.src_mask,src_key_padding_mask) # encode部分\n",
    "        \n",
    "        output = self.decoder(output) # decoder\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这个是计算训练的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,targets,padd_mask):\n",
    "    model.train()  #需要在训练时添加model.train()，在测试时添加model.eval()。其中model.train()是保证BN层用每一批数据的均值和方差\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "#     for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "#         data, targets, key_padding_mask = get_batch(train_data, i, batch_size)\n",
    "# 每循环一次进来得矩阵结构：data：torch.Size([45, 50, 1])   targets：torch.Size([45, 50, 1])   key_padding_mask：torch.Size([50, 45])\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    data1 = torch.tensor(data,dtype=torch.float32).to(device)\n",
    "    key_padding_mask1 = torch.tensor(padd_mask,dtype=torch.float32).to(device)\n",
    "#     print(data.dtype)\n",
    "#     print(key_padding_mask.dtype)\n",
    "    \n",
    "    output = model(data1,key_padding_mask1) # 传入模型得是 原始得数据 和 key_padding_mask\n",
    "\n",
    "#     if calculate_loss_over_all_values: # 前边定义了 ：calculate_loss_over_all_values = False\n",
    "#         loss = criterion(output, targets)\n",
    "#     else:\n",
    "    loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "#     print(\"train\",loss)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算在测试集上计算损失的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,data,target,padd_mask):\n",
    "    model.eval()  # 在对模型进行评估时，应该配合使用with torch.no_grad() 与 model.eval()：\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 50\n",
    "    with torch.no_grad():\n",
    "            output = model(data,padd_mask)\n",
    "#             if calculate_loss_over_all_values: #原本就是设置的flase\n",
    "#                 total_loss += len(data[0]) * criterion(output, target).cpu().item()\n",
    "#             else:\n",
    "            total_loss += len(data[0]) * criterion(output[-output_window:],targets[-output_window:]).cpu().item()\n",
    "    return total_loss #/ len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 5  # batch size\n",
    "torch.set_default_tensor_type(torch.DoubleTensor) # 直接设置创建的tensor类型默认为Double，如果不设置的话自动默认为float类型。\n",
    "input_window = 50\n",
    "output_window = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransAm().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "lr = 0.00001\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.96)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10 # The number of epochs\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 造数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分训练集和测试集 然后各自构造掩码 然后在进行训练的时候一边测试一边训练 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入数据的维度是 句子长度 batch大小  每个单词的维度 ， padding矩阵 是1的部分是mask掉的 ，不是0的部分是要训练可见的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 45, 1])\n",
      "torch.Size([50, 45, 1])\n",
      "torch.Size([45, 50])\n",
      "torch.Size([50, 45, 1])\n",
      "torch.Size([50, 45, 1])\n",
      "torch.Size([45, 50])\n"
     ]
    }
   ],
   "source": [
    "data=np.random.randn(2250)\n",
    "data=torch.tensor(data.reshape([50, 45, 1])).to(device)\n",
    "print(data.shape)\n",
    "\n",
    "targets=np.random.randn(2250)\n",
    "targets=torch.tensor(targets.reshape([50, 45, 1])).to(device)\n",
    "print(targets.shape)\n",
    "\n",
    "# numpy.zeros(3)\n",
    "key_padding_mask=np.zeros(2250)\n",
    "key_padding_mask=torch.tensor(key_padding_mask.reshape([45,50])).to(device)\n",
    "print(key_padding_mask.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_data=np.random.randn(2250)\n",
    "test_data=torch.tensor(test_data.reshape([50, 45, 1])).to(device)\n",
    "print(test_data.shape)\n",
    "\n",
    "test_targets=np.random.randn(2250)\n",
    "test_targets=torch.tensor(test_targets.reshape([50, 45, 1])).to(device)\n",
    "print(test_targets.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_key_padding_mask=np.zeros(2250)\n",
    "test_key_padding_mask=torch.tensor(test_key_padding_mask.reshape([45,50])).to(device)\n",
    "print(test_key_padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.96s | valid loss 149.33289 | train loss 152.49291 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.48s | valid loss 108.80559 | train loss 111.87309 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.47s | valid loss 79.94246 | train loss 82.45022 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.46s | valid loss 63.24713 | train loss 64.80456 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.48s | valid loss 56.97300 | train loss 57.28164 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.47s | valid loss 58.33046 | train loss 57.37866 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.48s | valid loss 61.33584 | train loss 59.68392 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.47s | valid loss 62.95046 | train loss 61.09201 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.47s | valid loss 62.44601 | train loss 60.72883 \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.47s | valid loss 60.27176 | train loss 58.86540 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(data,targets,key_padding_mask)\n",
    "    train_loss = evaluate(model,data,targets,key_padding_mask)\n",
    "    val_loss = evaluate(model,test_data,test_targets,test_key_padding_mask) \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | train loss {:5.5f} '.format(\n",
    "        epoch, (time.time() - epoch_start_time),\n",
    "        val_loss, train_loss))  # , math.exp(val_loss) | valid ppl {:8.2f}\n",
    "    print('-' * 89)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
